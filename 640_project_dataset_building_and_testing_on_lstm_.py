# -*- coding: utf-8 -*-
"""640 Project- DATASET BUILDING AND TESTING ON LSTM .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HUNS0t0FhxqACuENbMfIeADPm_WNt3YX
"""

!pip install mediapipe opencv-python scipy matplotlib seaborn scikit-learn
import os, numpy as np, pandas as pd

from google.colab import drive
drive.mount('/content/drive')

EMOKINE_FLDS = "/content/drive/MyDrive/EmokineDataset_v1.0/Stimuli/FLD"
EMOKINE_CSV  = "/content/drive/MyDrive/EmokineDataset_v1.0/Data/CSV"
# OUTPUT FOLDERS
PROCESSED_2D = "/content/drive/MyDrive/processed/pose2d/"
PROCESSED_3D = "/content/drive/MyDrive/processed/pose3d/"

os.makedirs(PROCESSED_2D, exist_ok=True)
os.makedirs(PROCESSED_3D, exist_ok=True)

"""MEDIA PIPE — 2D POSE EXTRACTION PIPELINE"""

import cv2
import mediapipe as mp

mp_pose = mp.solutions.pose

"""Function: Extract 2D Pose"""

def extract_pose_sequence(video_path):
    cap = cv2.VideoCapture(video_path)
    pose = mp_pose.Pose(static_image_mode=False)

    frames = []
    while True:
        ret, frame = cap.read()
        if not ret:
            break

        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        result = pose.process(rgb)

        if result.pose_landmarks:
            pts = np.array([[lm.x, lm.y] for lm in result.pose_landmarks.landmark])
        else:
            pts = np.zeros((33,2))

        frames.append(pts)

    cap.release()
    return np.array(frames)

"""Batch Process FLD videos"""

from tqdm import tqdm

def process_fld_videos(input_dir, out_dir):
    for file in tqdm(os.listdir(input_dir)):
        if file.endswith(".mp4"):
            seq = extract_pose_sequence(os.path.join(input_dir, file))
            np.save(os.path.join(out_dir, file.replace(".mp4", ".npy")), seq)

process_fld_videos(EMOKINE_FLDS, PROCESSED_2D)

"""3. CSV — 3D KEYPOINT PIPELINE

Load CSV 3D Keypoints
"""

def load_emokine_csv(path):
    df = pd.read_csv(path)
    data = df.iloc[:, 1:].values  # drop frame index
    num_frames = len(df)
    num_joints = data.shape[1]//3
    return data.reshape(num_frames, num_joints, 3)

"""Process CSV folder"""

def process_csv_folder(csv_dir, out_dir):
    for file in tqdm(os.listdir(csv_dir)):
        if file.endswith(".csv"):
            seq = load_emokine_csv(os.path.join(csv_dir, file))
            np.save(os.path.join(out_dir, file.replace(".csv", ".npy")), seq)

process_csv_folder(EMOKINE_CSV, PROCESSED_3D)

"""4. NORMALIZATION (APPLIES TO BOTH 2D + 3D)

Center, Scale, Resample
"""

from scipy.signal import resample

def center_sequence(seq):
    mid_hip = (seq[:,23] + seq[:,24]) / 2
    return seq - mid_hip[:,None,:]

def scale_sequence(seq):
    shoulder = np.linalg.norm(seq[:,11] - seq[:,12], axis=1)
    shoulder[shoulder==0] = 1e-6
    return seq / shoulder[:,None,None]

def resample_sequence(seq, target=128):
    return resample(seq, target, axis=0)

"""5. DATASET MERGING FOR LSTM

Label Mapping
"""

# Example mapping; adjust based on your data
label_map = {
    "anger":0,"disgust":1,"fear":2,"happy":3,
    "neutral":4,"sad":5,"surprise":6
}

"""Build Final Dataset"""

def build_dataset(pose2d_dir, pose3d_dir):
    X, y = [], []

    for file in os.listdir(pose2d_dir):
        seq = np.load(os.path.join(pose2d_dir, file))
        seq = resample_sequence(scale_sequence(center_sequence(seq)))
        X.append(seq.reshape(128,-1))

    for file in os.listdir(pose3d_dir):
        seq = np.load(os.path.join(pose3d_dir, file))
        seq = resample_sequence(scale_sequence(center_sequence(seq)))
        X.append(seq.reshape(128,-1))

    return np.array(X, dtype=np.float32), np.array(y)

X, y = build_dataset(PROCESSED_2D, PROCESSED_3D)
print("Final dataset:", X.shape)

"""6. BASELINE LSTM MODEL

Baseline LSTM Implementation
"""

import torch
import torch.nn as nn

class LSTMEmotion(nn.Module):
    def __init__(self, input_dim, hidden_dim=128, layers=2, classes=7):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, classes)

    def forward(self, x):
        out,_ = self.lstm(x)
        return self.fc(out[:,-1,:])

"""7. ATTENTION-ENHANCED LSTM MODEL

Attention model
"""

class AttentionLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim=128, layers=2, classes=7):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, layers, batch_first=True)
        self.attn = nn.Linear(hidden_dim, 1)
        self.fc = nn.Linear(hidden_dim, classes)

    def forward(self, x):
        out,_ = self.lstm(x)          # (B, T, H)
        scores = self.attn(out)       # (B, T, 1)
        weights = torch.softmax(scores, dim=1)
        ctx = torch.sum(weights * out, dim=1)
        return self.fc(ctx)

"""8. TRAINING LOOP

Preparedata loader
"""

print("X shape:", X.shape)
print("y shape:", y.shape)
print("len X:", len(X))
print("len y:", len(y))

import os

files_2d = os.listdir(PROCESSED_2D)
files_3d = os.listdir(PROCESSED_3D)

print("2D samples (first 10):", files_2d[:10])
print("3D samples (first 10):", files_3d[:10])

def get_label_from_2d_filename(file, label_map):
    # Example: "FLD_seq1_joy.npy"
    name = file.replace(".npy", "")
    parts = name.split("_")

    emotion = parts[-1]   # last part: joy, angry, fearful, etc.
    emotion = emotion.lower()

    if emotion in label_map:
        return label_map[emotion]

    print("Could not parse label from:", file)
    return None

label_map = {
    "angry": 0,
    "content": 1,     # you can map 'content' however you want
    "fearful": 2,
    "joy": 3,
    "neutral": 4,
    "sad": 5
}

def build_dataset_2d_only(pose2d_dir, label_map):
    X, y = [], []

    for file in os.listdir(pose2d_dir):
        if not file.endswith(".npy"):
            continue

        seq = np.load(os.path.join(pose2d_dir, file))

        label = get_label_from_2d_filename(file, label_map)
        if label is None:
            continue

        seq = resample_sequence(scale_sequence(center_sequence(seq)))
        X.append(seq.reshape(128, -1))
        y.append(label)

    print("Total X:", len(X))
    print("Total y:", len(y))
    return np.array(X, dtype=np.float32), np.array(y, dtype=np.int64)

X, y = build_dataset_2d_only(PROCESSED_2D, label_map)

import torch
from torch.utils.data import TensorDataset, DataLoader

X_tensor = torch.tensor(X)  # shape (54, 128, 66)
y_tensor = torch.tensor(y)

dataset = TensorDataset(X_tensor, y_tensor)
loader = DataLoader(dataset, batch_size=8, shuffle=True)

import torch.nn as nn

class LSTMEmotion(nn.Module):
    def __init__(self, input_dim, hidden=128, layers=2, classes=6):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden, layers, batch_first=True, dropout=0.2)
        self.fc = nn.Linear(hidden, classes)

    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1])

model = LSTMEmotion(input_dim=X.shape[2])
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()

for epoch in range(20):
    running_loss = 0
    for xb, yb in loader:
        optimizer.zero_grad()
        pred = model(xb)
        loss = criterion(pred, yb)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    print(f"Epoch {epoch+1} Loss: {running_loss:.4f}")

from sklearn.metrics import accuracy_score, f1_score, confusion_matrix

model.eval()
preds = model(X_tensor).argmax(1).numpy()
labels = y_tensor.numpy()

print("Accuracy:", accuracy_score(labels, preds))
print("Macro F1:", f1_score(labels, preds, average="macro"))
print(confusion_matrix(labels, preds))

import os
os.listdir(EMOKINE_CSV)[:10]

print(os.listdir(EMOKINE_CSV)[:10])

import os

csv_root = EMOKINE_CSV

for f in os.listdir(csv_root):
    print(f, "->", "FILE" if os.path.isfile(os.path.join(csv_root, f)) else "DIR")

for f in os.listdir(EMOKINE_CSV):
    print(f, "->", "FILE" if os.path.isfile(os.path.join(EMOKINE_CSV, f)) else "DIR")

first = os.listdir(EMOKINE_CSV)[0]
os.listdir(os.path.join(EMOKINE_CSV, first))

import pandas as pd
import os

first = os.listdir(EMOKINE_CSV)[0]
pos_path = os.path.join(EMOKINE_CSV, first, "position.csv")

df = pd.read_csv(pos_path)
print(df.head())
print(df.shape)

import numpy as np
import pandas as pd
import os

def load_3d_positions(folder_path):
    df = pd.read_csv(os.path.join(folder_path, "position.csv"))

    # drop metadata columns (frame_idx, ms)
    coords = df.iloc[:, 2:].values   # shape = (T, 69)

    T = coords.shape[0]
    J = coords.shape[1] // 3          # 69/3 = 23 joints

    seq = coords.reshape(T, J, 3)
    return seq

from tqdm import tqdm
import os
import numpy as np
import pandas as pd

def process_3d_folders(csv_root, out_dir):
    os.makedirs(out_dir, exist_ok=True)

    for folder in tqdm(os.listdir(csv_root)):
        folder_path = os.path.join(csv_root, folder)

        # Skip files, only process directories
        if not os.path.isdir(folder_path):
            continue

        # Skip explanation folder
        pos_file = os.path.join(folder_path, "position.csv")
        if not os.path.exists(pos_file):
            print(f"Skipping {folder} (position.csv not found)")
            continue

        # Load 3D joint positions
        df = pd.read_csv(pos_file)
        coords = df.iloc[:, 2:].values   # remove frame_idx, ms
        T = coords.shape[0]
        J = coords.shape[1] // 3
        seq = coords.reshape(T, J, 3)

        # Save npy
        np.save(os.path.join(out_dir, folder + ".npy"), seq)

len(os.listdir(PROCESSED_3D))

def build_dataset_3d_only(pose3d_dir, label_map):
    X, y = [], []

    for file in os.listdir(pose3d_dir):
        if not file.endswith(".npy"):
            continue

        seq = np.load(os.path.join(pose3d_dir, file))   # (T, J, 3)

        # Normalize
        seq = resample_sequence(scale_sequence(center_sequence(seq)))

        # Flatten to (128, feature_dim)
        seq = seq.reshape(128, -1)

        # Extract emotion label
        label = get_label_from_3d_filename(file, label_map)
        if label is None:
            print("Skipping file with no label:", file)
            continue

        X.append(seq)
        y.append(label)

    return np.array(X, dtype=np.float32), np.array(y, dtype=np.int64)

X3D, y3D = build_dataset_3d_only(PROCESSED_3D, label_map)

print("3D dataset:", X3D.shape, y3D.shape)
print("Total samples:", len(X3D))

ds3d = TensorDataset(torch.tensor(X3D), torch.tensor(y3D))
loader3d = DataLoader(ds3d, batch_size=8, shuffle=True)

model3d = LSTMEmotion(input_dim=X3D.shape[2])
optimizer = torch.optim.Adam(model3d.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()

for epoch in range(20):
    total_loss = 0
    for xb, yb in loader3d:
        optimizer.zero_grad()
        pred = model3d(xb)
        loss = criterion(pred, yb)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}: {total_loss:.4f}")

from sklearn.metrics import accuracy_score, confusion_matrix, f1_score

preds = model3d(torch.tensor(X3D)).argmax(1).numpy()
print("3D Accuracy:", accuracy_score(y3D, preds))
print("3D F1:", f1_score(y3D, preds, average="macro"))
print(confusion_matrix(y3D, preds))

X3D, y3D = build_dataset_3d_only(PROCESSED_3D, label_map)
print("3D:", X3D.shape, y3D.shape)

"""Train Function"""

def train(model, loader, epochs=20, lr=1e-3):
    criterion = nn.CrossEntropyLoss()
    opt = torch.optim.Adam(model.parameters(), lr=lr)

    for e in range(epochs):
        total = 0
        for xb, yb in loader:
            opt.zero_grad()
            out = model(xb)
            loss = criterion(out, yb)
            loss.backward()
            opt.step()
            total += loss.item()
        print(f"Epoch {e+1}: {total:.4f}")

"""9. HYPERPARAMETER TUNING"""

#Grid Search

def run_experiment(hidden_sizes=[64,128], lrs=[1e-3,5e-4]):
    for h in hidden_sizes:
        for lr in lrs:
            print(f"\nTesting: hidden={h}, lr={lr}")
            model = LSTMEmotion(X.shape[2], hidden_dim=h)
            train(model, loader, epochs=5, lr=lr)

"""10. EVALUATION

Metrics
"""

from sklearn.metrics import accuracy_score, f1_score, confusion_matrix

def evaluate(model):
    model.eval()
    preds = []
    with torch.no_grad():
        for xb, _ in loader:
            preds.extend(model(xb).argmax(1).cpu().numpy())

    print("Accuracy:", accuracy_score(y, preds))
    print("Macro-F1:", f1_score(y, preds, average="macro"))
    print(confusion_matrix(y, preds))





"""TRAIN 2D-ONLY LSTM"""

X2D, y2D = build_dataset_2d_only(PROCESSED_2D, label_map)

print("2D:", X2D.shape, y2D.shape)

ds2d = TensorDataset(torch.tensor(X2D), torch.tensor(y2D))
loader2d = DataLoader(ds2d, batch_size=8, shuffle=True)

model2d = LSTMEmotion(input_dim=X2D.shape[2])
optimizer2d = torch.optim.Adam(model2d.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()

for epoch in range(20):
    total_loss = 0
    for xb, yb in loader2d:
        optimizer2d.zero_grad()
        out = model2d(xb)
        loss = criterion(out, yb)
        loss.backward()
        optimizer2d.step()
        total_loss += loss.item()

    print(f"Epoch {epoch+1}: {total_loss:.4f}")

preds2d = model2d(torch.tensor(X2D)).argmax(1).numpy()

print("2D Accuracy:", accuracy_score(y2D, preds2d))
print("2D Macro-F1:", f1_score(y2D, preds2d, average="macro"))
print(confusion_matrix(y2D, preds2d))

X2D, y2D = build_dataset_2d_only(PROCESSED_2D, label_map)



"""3d training"""

ds3d = TensorDataset(torch.tensor(X3D), torch.tensor(y3D))
loader3d = DataLoader(ds3d, batch_size=8, shuffle=True)

model3d = LSTMEmotion(input_dim=X3D.shape[2])
optimizer3D = torch.optim.Adam(model3d.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()

for epoch in range(20):
    total_loss = 0
    for xb, yb in loader3d:
        optimizer3D.zero_grad()
        pred = model3d(xb)
        loss = criterion(pred, yb)
        loss.backward()
        optimizer3D.step()
        total_loss += loss.item()

    print(f"[3D] Epoch {epoch+1}, Loss={total_loss:.4f}")

preds3d = model3d(torch.tensor(X3D)).argmax(1).numpy()

print("3D Accuracy:", accuracy_score(y3D, preds3d))
print("3D Macro-F1:", f1_score(y3D, preds3d, average="macro"))
print("3D Confusion Matrix:\n", confusion_matrix(y3D, preds3d))



"""FEATURE ENGINEERING PIPELINE"""

import numpy as np

### Helper to reshape flat → joints
def unflatten(seq, is3d=False):
    T = seq.shape[0]
    if is3d:
        J = seq.shape[1] // 3
        return seq.reshape(T, J, 3)
    else:
        J = seq.shape[1] // 2
        return seq.reshape(T, J, 2)

### --- A. Velocity ---
def compute_velocity(seq):
    return np.diff(seq, axis=0)   # (T-1, J, D)

### --- B. Acceleration ---
def compute_acceleration(seq):
    vel = compute_velocity(seq)
    return np.diff(vel, axis=0)

### --- C. Smoothness (Mean squared jerk, simplified) ---
def compute_smoothness(seq):
    acc = compute_acceleration(seq)
    return np.mean(acc**2)

### --- D. Symmetry (Left vs Right) ---
# Works for both 2D and 3D
def compute_symmetry(seq):
    J = seq.shape[1]
    half = J // 2       # assume symmetric indexing L/R
    left  = seq[:, :half]
    right = seq[:, half:half*2]
    return np.mean(np.linalg.norm(left - right, axis=-1))

### --- E. Body Expansion (distance from root joint) ---
def compute_body_expansion(seq):
    root = seq[:, 0:1, :]                # Pelvis or mid-hip
    dist = np.linalg.norm(seq - root, axis=2)
    return np.mean(dist)

"""EXTRACT ALL FEATURES INTO A SINGLE VECTOR"""

def extract_features(seq_flat):
    # Determine 2D or 3D
    is3d = True if seq_flat.shape[1] % 3 == 0 else False
    seq = unflatten(seq_flat, is3d)

    # Compute features
    vel = compute_velocity(seq)
    acc = compute_acceleration(seq)

    smoothness = compute_smoothness(seq)
    symmetry   = compute_symmetry(seq)
    expansion  = compute_body_expansion(seq)

    # Aggregate simple stats from velocity/acceleration magnitudes
    vel_mag = np.linalg.norm(vel, axis=2)
    acc_mag = np.linalg.norm(acc, axis=2)

    features = [
        np.mean(vel_mag),
        np.std(vel_mag),
        np.mean(acc_mag),
        np.std(acc_mag),
        smoothness,
        symmetry,
        expansion
    ]

    return np.array(features)

"""BUILD FEATURE DATASETS FOR 2D AND 3D"""

def build_feature_dataset(X):
    feats = []
    for seq in X:
        feats.append(extract_features(seq))
    return np.array(feats)

X2D_feat = build_feature_dataset(X2D)
X3D_feat = build_feature_dataset(X3D)

print(X2D_feat.shape)   # (54, 7)
print(X3D_feat.shape)   # (54, 7)

"""BASELINE CLASSICAL MODELS

TRAIN SVM (2D Engineered Features)
"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix

svm2d = SVC(kernel='rbf', C=5, gamma='scale')
svm2d.fit(X2D_feat, y2D)

pred2d = svm2d.predict(X2D_feat)

print("SVM (2D) Accuracy:", accuracy_score(y2D, pred2d))
print("SVM (2D) F1:", f1_score(y2D, pred2d, average="macro"))
print(confusion_matrix(y2D, pred2d))

"""TRAIN SVM (3D Engineered Features)"""

svm3d = SVC(kernel='rbf', C=5, gamma='scale')
svm3d.fit(X3D_feat, y3D)

pred3d = svm3d.predict(X3D_feat)

print("SVM (3D) Accuracy:", accuracy_score(y3D, pred3d))
print("SVM (3D) F1:", f1_score(y3D, pred3d, average="macro"))
print(confusion_matrix(y3D, pred3d))

"""ATTENTION-ENHANCED LSTM

Attention LSTM Model
"""

class AttentionLSTM(nn.Module):
    def __init__(self, input_dim, hidden=128, layers=2, classes=6):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden, layers, batch_first=True)
        self.attn = nn.Linear(hidden, 1)     # attention scores
        self.fc = nn.Linear(hidden, classes)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)                 # (B, T, H)
        scores = self.attn(lstm_out)               # (B, T, 1)
        weights = torch.softmax(scores, dim=1)     # (B, T, 1)
        context = torch.sum(weights * lstm_out, dim=1)  # (B, H)
        return self.fc(context)

"""Training Attention LSTM (2D Example)"""

att2d = AttentionLSTM(input_dim=X2D.shape[2])
opt = torch.optim.Adam(att2d.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()

for epoch in range(20):
    total_loss = 0
    for xb, yb in loader2d:
        opt.zero_grad()
        pred = att2d(xb)
        loss = criterion(pred, yb)
        loss.backward()
        opt.step()
        total_loss += loss.item()
    print("Epoch", epoch+1, "Loss:", total_loss)

pred = att2d(torch.tensor(X2D)).argmax(1).numpy()
print("Attention-LSTM 2D Accuracy:", accuracy_score(y2D, pred))
print("F1:", f1_score(y2D, pred, average="macro"))

att3d = AttentionLSTM(input_dim=X3D.shape[2])

"""data loader for 3d"""

ds3d = TensorDataset(torch.tensor(X3D), torch.tensor(y3D))
loader3d = DataLoader(ds3d, batch_size=8, shuffle=True)

"""defining optimizer loss"""

optimizer3d = torch.optim.Adam(att3d.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()

"""Train Attention 3d"""

num_epochs = 20

for epoch in range(num_epochs):
    att3d.train()
    total_loss = 0

    for xb, yb in loader3d:
        optimizer3d.zero_grad()
        preds = att3d(xb)
        loss = criterion(preds, yb)
        loss.backward()
        optimizer3d.step()
        total_loss += loss.item()

    print(f"[3D-Attention] Epoch {epoch+1}/{num_epochs} - Loss: {total_loss:.4f}")

"""Evaluate the Attention-LSTM (3D)"""

from sklearn.metrics import accuracy_score, f1_score, confusion_matrix

att3d.eval()
with torch.no_grad():
    pred3d = att3d(torch.tensor(X3D)).argmax(1).numpy()

print("3D Attention-LSTM Accuracy:", accuracy_score(y3D, pred3d))
print("3D Attention-LSTM Macro-F1:", f1_score(y3D, pred3d, average='macro'))
print("3D Attention-LSTM Confusion Matrix:\n", confusion_matrix(y3D, pred3d))

"""Save model checkpoints"""

torch.save(att3d.state_dict(), "attention_lstm_3d.pth")

