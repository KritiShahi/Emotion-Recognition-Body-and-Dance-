{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAfDuTJy4fxg",
        "outputId": "78addbb5-ebc4-43ea-abb1-bc65acdcd2b1"
      },
      "id": "PAfDuTJy4fxg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Kinematic Emotion Recognition Dataset - Data Setup & Feature Extraction\n",
        "Complete data processing pipeline for BVH motion capture files\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from pathlib import Path\n",
        "import re\n",
        "from typing import List, Dict, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "class BVHParser:\n",
        "    \"\"\"Parse BVH (Biovision Hierarchy) motion capture files\"\"\"\n",
        "\n",
        "    def __init__(self, filepath):\n",
        "        self.filepath = filepath\n",
        "        self.joints = []\n",
        "        self.hierarchy = {}\n",
        "        self.motion_data = None\n",
        "        self.frame_time = 0.0\n",
        "        self.num_frames = 0\n",
        "        self.channels_list = []\n",
        "        self.joint_parents = {}\n",
        "\n",
        "    def parse(self):\n",
        "        \"\"\"Parse BVH file\"\"\"\n",
        "        with open(self.filepath, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "\n",
        "        # Find MOTION section\n",
        "        motion_idx = None\n",
        "        for i, line in enumerate(lines):\n",
        "            if 'MOTION' in line.upper():\n",
        "                motion_idx = i\n",
        "                break\n",
        "\n",
        "        # Parse sections\n",
        "        self._parse_hierarchy(lines[:motion_idx])\n",
        "        self._parse_motion(lines[motion_idx:])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _parse_hierarchy(self, lines):\n",
        "        \"\"\"Extract joint hierarchy\"\"\"\n",
        "        joint_stack = []\n",
        "        current_joint = None\n",
        "\n",
        "        for line in lines:\n",
        "            stripped = line.strip()\n",
        "\n",
        "            if stripped.startswith('ROOT') or stripped.startswith('JOINT'):\n",
        "                parts = stripped.split()\n",
        "                if len(parts) >= 2:\n",
        "                    current_joint = parts[1]\n",
        "                    self.joints.append(current_joint)\n",
        "                    self.hierarchy[current_joint] = {\n",
        "                        'channels': [],\n",
        "                        'offset': [0, 0, 0],\n",
        "                        'children': []\n",
        "                    }\n",
        "\n",
        "                    if joint_stack:\n",
        "                        parent = joint_stack[-1]\n",
        "                        self.joint_parents[current_joint] = parent\n",
        "                        self.hierarchy[parent]['children'].append(current_joint)\n",
        "\n",
        "                    joint_stack.append(current_joint)\n",
        "\n",
        "            elif stripped.startswith('End Site'):\n",
        "                # End sites don't have channels, skip them\n",
        "                pass\n",
        "\n",
        "            elif stripped.startswith('OFFSET') and current_joint:\n",
        "                parts = stripped.split()\n",
        "                if len(parts) >= 4:\n",
        "                    try:\n",
        "                        offset = [float(parts[1]), float(parts[2]), float(parts[3])]\n",
        "                        self.hierarchy[current_joint]['offset'] = offset\n",
        "                    except ValueError:\n",
        "                        pass\n",
        "\n",
        "            elif stripped.startswith('CHANNELS') and current_joint:\n",
        "                parts = stripped.split()\n",
        "                if len(parts) >= 2:\n",
        "                    try:\n",
        "                        num_channels = int(parts[1])\n",
        "                        channels = parts[2:2+num_channels]\n",
        "                        self.hierarchy[current_joint]['channels'] = channels\n",
        "                        for ch in channels:\n",
        "                            self.channels_list.append((current_joint, ch))\n",
        "                    except (ValueError, IndexError):\n",
        "                        pass\n",
        "\n",
        "            elif '}' in stripped and joint_stack:\n",
        "                joint_stack.pop()\n",
        "                if joint_stack:\n",
        "                    current_joint = joint_stack[-1]\n",
        "\n",
        "    def _parse_motion(self, lines):\n",
        "        \"\"\"Extract motion data\"\"\"\n",
        "        # Parse header\n",
        "        for line in lines[:10]:\n",
        "            if 'Frames:' in line:\n",
        "                    self.num_frames = int(line.split(':')[1].strip())\n",
        "            elif 'Frame Time:' in line:\n",
        "                    self.frame_time = float(line.split(':')[1].strip())\n",
        "\n",
        "        # Find where actual data starts\n",
        "        data_start = 0\n",
        "        for i, line in enumerate(lines):\n",
        "            if line.strip() and not any(keyword in line for keyword in ['MOTION', 'Frames', 'Frame Time']):\n",
        "                # This line should be numeric data\n",
        "                try:\n",
        "                    float(line.strip().split()[0])\n",
        "                    data_start = i\n",
        "                    break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        # Read motion data\n",
        "        data = []\n",
        "        for line in lines[data_start:]:\n",
        "            stripped = line.strip()\n",
        "            if stripped:\n",
        "                try:\n",
        "                    values = [float(x) for x in stripped.split()]\n",
        "                    if len(values) > 0:\n",
        "                        data.append(values)\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "        if data:\n",
        "            self.motion_data = np.array(data)\n",
        "            if self.num_frames == 0:\n",
        "                self.num_frames = len(data)\n",
        "        else:\n",
        "            raise Exception(\"No motion data found\")\n",
        "\n",
        "    def get_joint_channels(self, joint_name):\n",
        "        \"\"\"Get data for specific joint across all frames\"\"\"\n",
        "        if joint_name not in self.hierarchy:\n",
        "            return None\n",
        "\n",
        "        if self.motion_data is None:\n",
        "            return None\n",
        "\n",
        "        # Find column indices for this joint\n",
        "        col_idx = 0\n",
        "        for joint in self.joints:\n",
        "            if joint == joint_name:\n",
        "                num_channels = len(self.hierarchy[joint]['channels'])\n",
        "                if num_channels > 0:\n",
        "                    return self.motion_data[:, col_idx:col_idx+num_channels]\n",
        "                else:\n",
        "                    return None\n",
        "            col_idx += len(self.hierarchy[joint]['channels'])\n",
        "\n",
        "        return None\n",
        "\n",
        "    def get_joint_position_indices(self, joint_name):\n",
        "        \"\"\"Get indices of position channels for a joint\"\"\"\n",
        "        if joint_name not in self.hierarchy:\n",
        "            return []\n",
        "\n",
        "        channels = self.hierarchy[joint_name]['channels']\n",
        "        position_indices = []\n",
        "\n",
        "        for i, ch in enumerate(channels):\n",
        "            if 'position' in ch.lower():\n",
        "                position_indices.append(i)\n",
        "\n",
        "        return position_indices\n",
        "\n",
        "# ============================================================================\n",
        "# FEATURE EXTRACTION\n",
        "# ============================================================================\n",
        "\n",
        "class KinematicFeatureExtractor:\n",
        "    \"\"\"Extract features from BVH data\"\"\"\n",
        "\n",
        "    def __init__(self, bvh_parser):\n",
        "        self.parser = bvh_parser\n",
        "        self.features = {}\n",
        "\n",
        "    def extract_all_features(self):\n",
        "        \"\"\"Extract comprehensive feature set\"\"\"\n",
        "        self.features = {}\n",
        "\n",
        "        # Get all joints with position data\n",
        "        position_data = {}\n",
        "\n",
        "        for joint in self.parser.joints:\n",
        "            channels = self.parser.get_joint_channels(joint)\n",
        "            if channels is not None:\n",
        "                pos_indices = self.parser.get_joint_position_indices(joint)\n",
        "\n",
        "                if len(pos_indices) == 3:  # Full 3D position\n",
        "                    position_data[joint] = channels[:, pos_indices]\n",
        "                elif len(pos_indices) > 0:  # Partial position data\n",
        "                    position_data[joint] = channels[:, pos_indices]\n",
        "\n",
        "        if not position_data:\n",
        "            # If no position data, use all channel data\n",
        "            for joint in self.parser.joints:\n",
        "                channels = self.parser.get_joint_channels(joint)\n",
        "                if channels is not None and len(channels) > 0:\n",
        "                    position_data[joint] = channels\n",
        "\n",
        "        # Extract features from available data\n",
        "        self._extract_movement_features(position_data)\n",
        "        self._extract_statistical_features(position_data)\n",
        "\n",
        "        return self.features\n",
        "\n",
        "    def _extract_movement_features(self, position_data):\n",
        "        \"\"\"Extract velocity and acceleration features\"\"\"\n",
        "\n",
        "        for joint_name, data in position_data.items():\n",
        "            if len(data) < 2:\n",
        "                continue\n",
        "\n",
        "            # Calculate velocity\n",
        "            velocity = np.diff(data, axis=0)\n",
        "            speed = np.linalg.norm(velocity, axis=1) if velocity.shape[1] > 1 else np.abs(velocity).flatten()\n",
        "\n",
        "            if len(speed) > 0:\n",
        "                self.features[f'{joint_name}_speed_mean'] = float(np.mean(speed))\n",
        "                self.features[f'{joint_name}_speed_std'] = float(np.std(speed))\n",
        "                self.features[f'{joint_name}_speed_max'] = float(np.max(speed))\n",
        "\n",
        "            # Calculate acceleration\n",
        "            if len(data) >= 3:\n",
        "                acceleration = np.diff(velocity, axis=0)\n",
        "                acc_mag = np.linalg.norm(acceleration, axis=1) if acceleration.shape[1] > 1 else np.abs(acceleration).flatten()\n",
        "\n",
        "                if len(acc_mag) > 0:\n",
        "                    self.features[f'{joint_name}_acc_mean'] = float(np.mean(acc_mag))\n",
        "\n",
        "    def _extract_statistical_features(self, position_data):\n",
        "        \"\"\"Extract statistical features\"\"\"\n",
        "\n",
        "        all_speeds = []\n",
        "\n",
        "        for joint_name, data in position_data.items():\n",
        "            if len(data) < 2:\n",
        "                continue\n",
        "\n",
        "            # Range of motion\n",
        "            data_range = np.ptp(data, axis=0)  # Peak-to-peak\n",
        "            self.features[f'{joint_name}_range'] = float(np.mean(data_range))\n",
        "\n",
        "            # Velocity for overall statistics\n",
        "            velocity = np.diff(data, axis=0)\n",
        "            speed = np.linalg.norm(velocity, axis=1) if velocity.shape[1] > 1 else np.abs(velocity).flatten()\n",
        "            all_speeds.extend(speed)\n",
        "\n",
        "        # Overall movement statistics\n",
        "        if all_speeds:\n",
        "            self.features['overall_speed_mean'] = float(np.mean(all_speeds))\n",
        "            self.features['overall_speed_std'] = float(np.std(all_speeds))\n",
        "            self.features['overall_speed_max'] = float(np.max(all_speeds))\n",
        "            self.features['movement_intensity'] = float(np.percentile(all_speeds, 90))\n",
        "\n",
        "# ============================================================================\n",
        "# DATASET PROCESSOR\n",
        "# ============================================================================\n",
        "\n",
        "class EmotionDatasetProcessor:\n",
        "    \"\"\"Process emotion dataset\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, fileinfo_csv):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.fileinfo = pd.read_csv(fileinfo_csv)\n",
        "        self.features_df = None\n",
        "\n",
        "    def find_bvh_file(self, filename):\n",
        "        \"\"\"Try multiple locations to find BVH file\"\"\"\n",
        "        possible_paths = [\n",
        "            self.data_dir / f\"{filename}.bvh\",\n",
        "            self.data_dir / \"bvh\" / f\"{filename}.bvh\",\n",
        "            self.data_dir / \"data\" / f\"{filename}.bvh\",\n",
        "            self.data_dir / filename / f\"{filename}.bvh\",\n",
        "        ]\n",
        "\n",
        "        # Also search recursively\n",
        "        for bvh_path in self.data_dir.rglob(f\"{filename}.bvh\"):\n",
        "            return bvh_path\n",
        "\n",
        "        for path in possible_paths:\n",
        "            if path.exists():\n",
        "                return path\n",
        "\n",
        "        return None\n",
        "\n",
        "    def process_dataset(self, sample_size=None, save_path='features_dataset.csv'):\n",
        "        \"\"\"Process BVH files and extract features\"\"\"\n",
        "\n",
        "        print(f\"Processing dataset with {len(self.fileinfo)} entries...\")\n",
        "\n",
        "        if sample_size:\n",
        "            print(f\"Sampling {sample_size} files\")\n",
        "            fileinfo_sample = self.fileinfo.sample(n=min(sample_size, len(self.fileinfo)), random_state=42)\n",
        "        else:\n",
        "            fileinfo_sample = self.fileinfo\n",
        "\n",
        "        all_features = []\n",
        "        failed_files = []\n",
        "        error_details = {}\n",
        "\n",
        "        for idx, row in fileinfo_sample.iterrows():\n",
        "            filename = row['filename']\n",
        "\n",
        "            # Find BVH file\n",
        "            bvh_path = self.find_bvh_file(filename)\n",
        "\n",
        "            if bvh_path is None:\n",
        "                failed_files.append(filename)\n",
        "                error_details[filename] = \"File not found\"\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Parse BVH\n",
        "                parser = BVHParser(str(bvh_path))\n",
        "                parser.parse()\n",
        "\n",
        "                # Extract features\n",
        "                extractor = KinematicFeatureExtractor(parser)\n",
        "                features = extractor.extract_all_features()\n",
        "\n",
        "                if not features:\n",
        "                    failed_files.append(filename)\n",
        "                    error_details[filename] = \"No features extracted\"\n",
        "                    continue\n",
        "\n",
        "                # Add metadata\n",
        "                features['filename'] = filename\n",
        "                features['actor_ID'] = row['actor_ID']\n",
        "                features['emotion'] = row['emotion']\n",
        "                features['gender'] = row['actor_gender']\n",
        "                features['num_frames'] = parser.num_frames\n",
        "                features['duration'] = parser.num_frames * parser.frame_time\n",
        "\n",
        "                all_features.append(features)\n",
        "\n",
        "                if (len(all_features)) % 20 == 0:\n",
        "                    print(f\"Processed {len(all_features)} files successfully...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                failed_files.append(filename)\n",
        "                error_details[filename] = str(e)\n",
        "\n",
        "        if all_features:\n",
        "            self.features_df = pd.DataFrame(all_features)\n",
        "            self.features_df.to_csv(save_path, index=False)\n",
        "\n",
        "            print(f\"Successfully processed: {len(all_features)} files\")\n",
        "            print(f\"Features saved to: {save_path}\")\n",
        "            print(f\"Total features per sample: {len([c for c in self.features_df.columns if c not in ['filename', 'actor_ID', 'emotion', 'gender', 'num_frames', 'duration']])}\")\n",
        "\n",
        "            if failed_files:\n",
        "                print(f\"\\n Failed files: {len(failed_files)}\")\n",
        "                print(\"\\n First 5 errors:\")\n",
        "                for filename in list(error_details.keys())[:5]:\n",
        "                    print(f\"  - {filename}: {error_details[filename]}\")\n",
        "\n",
        "            return self.features_df\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    print(\"KINEMATIC EMOTION RECOGNITION - DIAGNOSTIC & SETUP\")\n",
        "\n",
        "    # Configuration\n",
        "    data_dir = \"/content/drive/MyDrive/kinematic_dataset_final/BVH/\"\n",
        "    fileinfo_csv = \"/content/drive/MyDrive/kinematic_dataset_final/file-info.csv\"\n",
        "\n",
        "    sample_size = 1402\n",
        "\n",
        "    processor = EmotionDatasetProcessor(data_dir, fileinfo_csv)\n",
        "    features_df = processor.process_dataset(sample_size=sample_size)\n",
        "\n",
        "    if features_df is not None:\n",
        "        print(\"DATASET SUMMARY\")\n",
        "        print(f\"\\nTotal samples: {len(features_df)}\")\n",
        "        print(f\"\\nEmotion distribution:\")\n",
        "        print(features_df['emotion'].value_counts())\n",
        "        print(f\"\\nFeature count: {len(features_df.columns) - 6}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e2JOfN5KhvQ",
        "outputId": "005a268c-373f-4125-bb02-8b0b521ba8f7"
      },
      "id": "-e2JOfN5KhvQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KINEMATIC EMOTION RECOGNITION - DIAGNOSTIC & SETUP\n",
            "Processing dataset with 1402 entries...\n",
            "Sampling 1402 files\n",
            "Processed 20 files successfully...\n",
            "Processed 40 files successfully...\n",
            "Processed 60 files successfully...\n",
            "Processed 80 files successfully...\n",
            "Processed 100 files successfully...\n",
            "Processed 120 files successfully...\n",
            "Processed 140 files successfully...\n",
            "Processed 160 files successfully...\n",
            "Processed 180 files successfully...\n",
            "Processed 200 files successfully...\n",
            "Processed 220 files successfully...\n",
            "Processed 240 files successfully...\n",
            "Processed 260 files successfully...\n",
            "Processed 280 files successfully...\n",
            "Processed 300 files successfully...\n",
            "Processed 320 files successfully...\n",
            "Processed 340 files successfully...\n",
            "Processed 360 files successfully...\n",
            "Processed 380 files successfully...\n",
            "Processed 400 files successfully...\n",
            "Processed 420 files successfully...\n",
            "Processed 440 files successfully...\n",
            "Processed 460 files successfully...\n",
            "Processed 480 files successfully...\n",
            "Processed 500 files successfully...\n",
            "Processed 520 files successfully...\n",
            "Processed 540 files successfully...\n",
            "Processed 560 files successfully...\n",
            "Processed 580 files successfully...\n",
            "Processed 600 files successfully...\n",
            "Processed 620 files successfully...\n",
            "Processed 640 files successfully...\n",
            "Processed 660 files successfully...\n",
            "Processed 680 files successfully...\n",
            "Processed 700 files successfully...\n",
            "Processed 720 files successfully...\n",
            "Processed 740 files successfully...\n",
            "Processed 760 files successfully...\n",
            "Processed 780 files successfully...\n",
            "Processed 800 files successfully...\n",
            "Processed 820 files successfully...\n",
            "Processed 840 files successfully...\n",
            "Processed 860 files successfully...\n",
            "Processed 880 files successfully...\n",
            "Processed 900 files successfully...\n",
            "Processed 920 files successfully...\n",
            "Processed 940 files successfully...\n",
            "Processed 960 files successfully...\n",
            "Processed 980 files successfully...\n",
            "Processed 1000 files successfully...\n",
            "Processed 1020 files successfully...\n",
            "Processed 1040 files successfully...\n",
            "Processed 1060 files successfully...\n",
            "Processed 1080 files successfully...\n",
            "Processed 1100 files successfully...\n",
            "Processed 1120 files successfully...\n",
            "Processed 1140 files successfully...\n",
            "Processed 1160 files successfully...\n",
            "Processed 1180 files successfully...\n",
            "Processed 1200 files successfully...\n",
            "Processed 1220 files successfully...\n",
            "Processed 1240 files successfully...\n",
            "Processed 1260 files successfully...\n",
            "Processed 1280 files successfully...\n",
            "Processed 1300 files successfully...\n",
            "Processed 1320 files successfully...\n",
            "Processed 1340 files successfully...\n",
            "Processed 1360 files successfully...\n",
            "Processed 1380 files successfully...\n",
            "Processed 1400 files successfully...\n",
            "Successfully processed: 1401 files\n",
            "Features saved to: features_dataset.csv\n",
            "Total features per sample: 299\n",
            "\n",
            " Failed files: 1\n",
            "\n",
            " First 5 errors:\n",
            "  - M08F0V2: File not found\n",
            "\n",
            "============================================================\n",
            "DATASET SUMMARY\n",
            "\n",
            "Total samples: 1401\n",
            "\n",
            "Emotion distribution:\n",
            "emotion\n",
            "Fearful     216\n",
            "Happy       216\n",
            "Surprise    212\n",
            "Disgust     210\n",
            "Sad         202\n",
            "Angry       200\n",
            "Neutral     145\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Feature count: 299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix,\n",
        "    f1_score, precision_score, recall_score, accuracy_score,\n",
        "    precision_recall_fscore_support\n",
        ")\n",
        "from sklearn.impute import SimpleImputer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better-looking plots\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "\n",
        "class EmotionDataLoader:\n",
        "    \"\"\"Load and preprocess emotion recognition dataset\"\"\"\n",
        "\n",
        "    def __init__(self, features_csv):\n",
        "        self.features_csv = features_csv\n",
        "        self.df = None\n",
        "        self.X = None\n",
        "        self.y = None\n",
        "        self.feature_names = None\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load features from CSV\"\"\"\n",
        "\n",
        "        self.df = pd.read_csv(self.features_csv)\n",
        "        print(f\"\\n Loaded {len(self.df)} samples\")\n",
        "        print(f\"Total columns: {len(self.df.columns)}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def prepare_features(self):\n",
        "        \"\"\"Prepare feature matrix and labels\"\"\"\n",
        "\n",
        "        # Identify metadata columns to exclude\n",
        "        metadata_cols = ['filename', 'actor_ID', 'emotion', 'gender',\n",
        "                        'scenario_ID', 'version', 'num_frames', 'duration']\n",
        "\n",
        "        # Get feature columns\n",
        "        feature_cols = [col for col in self.df.columns if col not in metadata_cols]\n",
        "        self.feature_names = feature_cols\n",
        "\n",
        "        print(f\"\\n Feature columns: {len(feature_cols)}\")\n",
        "\n",
        "        # Extract features and labels\n",
        "        X = self.df[feature_cols].values\n",
        "        y = self.df['emotion'].values\n",
        "\n",
        "        # Encode labels\n",
        "        print(f\"\\n Encoding emotion labels...\")\n",
        "        y_encoded = self.label_encoder.fit_transform(y)\n",
        "\n",
        "        print(f\"\\n Emotion classes:\")\n",
        "        for i, emotion in enumerate(self.label_encoder.classes_):\n",
        "            count = np.sum(y == emotion)\n",
        "            print(f\"  {i}: {emotion} ({count} samples)\")\n",
        "\n",
        "        self.X = X\n",
        "        self.y = y_encoded\n",
        "\n",
        "        return self\n",
        "\n",
        "    def normalize_features(self, X_train, X_test):\n",
        "        \"\"\"Normalize features using training set statistics\"\"\"\n",
        "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "        X_test_scaled = self.scaler.transform(X_test)\n",
        "        return X_train_scaled, X_test_scaled\n",
        "\n",
        "    def get_emotion_name(self, encoded_label):\n",
        "        \"\"\"Convert encoded label back to emotion name\"\"\"\n",
        "        return self.label_encoder.inverse_transform([encoded_label])[0]\n",
        "\n",
        "class EmotionClassifierTrainer:\n",
        "    \"\"\"Train and evaluate multiple classifiers\"\"\"\n",
        "\n",
        "    def __init__(self, X, y, feature_names, label_encoder):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.feature_names = feature_names\n",
        "        self.label_encoder = label_encoder\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "\n",
        "    def split_data(self, test_size=0.2, random_state=42):\n",
        "        \"\"\"Split data into train and test sets\"\"\"\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            self.X, self.y, test_size=test_size, random_state=random_state, stratify=self.y\n",
        "        )\n",
        "\n",
        "\n",
        "        print(f\"\\n Training samples: {len(self.X_train)}\")\n",
        "        print(f\" Test samples: {len(self.X_test)}\")\n",
        "        print(f\" Test size: {test_size*100:.0f}%\")\n",
        "\n",
        "        # Normalize features\n",
        "        scaler = StandardScaler()\n",
        "        self.X_train = scaler.fit_transform(self.X_train)\n",
        "        self.X_test = scaler.transform(self.X_test)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def train_random_forest(self, n_estimators=100, max_depth=None):\n",
        "        \"\"\"Train Random Forest classifier\"\"\"\n",
        "\n",
        "        rf = RandomForestClassifier(\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=max_depth,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        print(f\"\\n→ Training with {n_estimators} trees...\")\n",
        "        rf.fit(self.X_train, self.y_train)\n",
        "\n",
        "        # Predictions\n",
        "        y_pred = rf.predict(self.X_test)\n",
        "\n",
        "        # Store results\n",
        "        self.models['Random Forest'] = rf\n",
        "        self.results['Random Forest'] = {\n",
        "            'model': rf,\n",
        "            'y_pred': y_pred,\n",
        "            'accuracy': accuracy_score(self.y_test, y_pred),\n",
        "            'precision': precision_score(self.y_test, y_pred, average='weighted'),\n",
        "            'recall': recall_score(self.y_test, y_pred, average='weighted'),\n",
        "            'f1': f1_score(self.y_test, y_pred, average='weighted')\n",
        "        }\n",
        "        print(f\" Results of Random Forest Classifier\")\n",
        "        print(f\"  Accuracy: {self.results['Random Forest']['accuracy']:.4f}\")\n",
        "\n",
        "        return rf\n",
        "\n",
        "    def train_svm(self, kernel='rbf', C=1.0):\n",
        "        \"\"\"Train SVM classifier\"\"\"\n",
        "\n",
        "        svm = SVC(\n",
        "            kernel=kernel,\n",
        "            C=C,\n",
        "            random_state=42,\n",
        "            probability=True\n",
        "        )\n",
        "\n",
        "        print(f\"\\n→ Training with {kernel} kernel...\")\n",
        "        svm.fit(self.X_train, self.y_train)\n",
        "\n",
        "        # Predictions\n",
        "        y_pred = svm.predict(self.X_test)\n",
        "\n",
        "        # Store results\n",
        "        self.models['SVM'] = svm\n",
        "        self.results['SVM'] = {\n",
        "            'model': svm,\n",
        "            'y_pred': y_pred,\n",
        "            'accuracy': accuracy_score(self.y_test, y_pred),\n",
        "            'precision': precision_score(self.y_test, y_pred, average='weighted'),\n",
        "            'recall': recall_score(self.y_test, y_pred, average='weighted'),\n",
        "            'f1': f1_score(self.y_test, y_pred, average='weighted')\n",
        "        }\n",
        "        print(f\" Results of SVM Classifier\")\n",
        "        print(f\"  Accuracy: {self.results['SVM']['accuracy']:.4f}\")\n",
        "\n",
        "        return svm\n",
        "\n",
        "    def train_gradient_boosting(self, n_estimators=100, learning_rate=0.1):\n",
        "        \"\"\"Train Gradient Boosting classifier\"\"\"\n",
        "\n",
        "        gb = GradientBoostingClassifier(\n",
        "            n_estimators=n_estimators,\n",
        "            learning_rate=learning_rate,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        print(f\"\\n→ Training with {n_estimators} estimators...\")\n",
        "        gb.fit(self.X_train, self.y_train)\n",
        "\n",
        "        # Predictions\n",
        "        y_pred = gb.predict(self.X_test)\n",
        "\n",
        "        # Store results\n",
        "        self.models['Gradient Boosting'] = gb\n",
        "        self.results['Gradient Boosting'] = {\n",
        "            'model': gb,\n",
        "            'y_pred': y_pred,\n",
        "            'accuracy': accuracy_score(self.y_test, y_pred),\n",
        "            'precision': precision_score(self.y_test, y_pred, average='weighted'),\n",
        "            'recall': recall_score(self.y_test, y_pred, average='weighted'),\n",
        "            'f1': f1_score(self.y_test, y_pred, average='weighted')\n",
        "        }\n",
        "        print(f\" Results of Gradient Boosting Classifier\")\n",
        "        print(f\"  Accuracy: {self.results['Gradient Boosting']['accuracy']:.4f}\")\n",
        "\n",
        "        return gb\n",
        "\n",
        "    def train_all_models(self):\n",
        "        \"\"\"Train all classifiers\"\"\"\n",
        "        self.train_random_forest(n_estimators=100)\n",
        "        self.train_svm(kernel='rbf', C=1.0)\n",
        "        self.train_gradient_boosting(n_estimators=100)\n",
        "        return self\n",
        "\n",
        "\n",
        "class ModelEvaluator:\n",
        "    \"\"\"Evaluate and visualize model performance\"\"\"\n",
        "\n",
        "    def __init__(self, trainer):\n",
        "        self.trainer = trainer\n",
        "        self.results = trainer.results\n",
        "        self.label_encoder = trainer.label_encoder\n",
        "\n",
        "    def print_comparison(self):\n",
        "        \"\"\"Print comparison of all models\"\"\"\n",
        "\n",
        "        comparison_df = pd.DataFrame({\n",
        "            'Model': list(self.results.keys()),\n",
        "            'Accuracy': [r['accuracy'] for r in self.results.values()],\n",
        "            'Precision': [r['precision'] for r in self.results.values()],\n",
        "            'Recall': [r['recall'] for r in self.results.values()],\n",
        "            'F1-Score': [r['f1'] for r in self.results.values()]\n",
        "        })\n",
        "\n",
        "        print(\"\\n\" + comparison_df.to_string(index=False))\n",
        "\n",
        "        # Find best model\n",
        "        best_model = comparison_df.loc[comparison_df['F1-Score'].idxmax(), 'Model']\n",
        "        best_f1 = comparison_df.loc[comparison_df['F1-Score'].idxmax(), 'F1-Score']\n",
        "\n",
        "        print(f\"\\n Best Model: {best_model} (F1-Score: {best_f1:.4f})\")\n",
        "\n",
        "        return comparison_df\n",
        "\n",
        "    def plot_metric_comparison(self, save_path='metric_comparison.png'):\n",
        "        \"\"\"Plot comparison of metrics across models\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "        fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
        "\n",
        "        models = list(self.results.keys())\n",
        "        metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
        "        metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "        colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
        "\n",
        "        for idx, (metric, metric_name) in enumerate(zip(metrics, metric_names)):\n",
        "            ax = axes[idx // 2, idx % 2]\n",
        "\n",
        "            values = [self.results[model][metric] for model in models]\n",
        "            bars = ax.bar(models, values, color=colors, alpha=0.8, edgecolor='black')\n",
        "\n",
        "            # Add value labels on bars\n",
        "            for bar in bars:\n",
        "                height = bar.get_height()\n",
        "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                       f'{height:.3f}',\n",
        "                       ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "            ax.set_ylabel(metric_name, fontsize=12, fontweight='bold')\n",
        "            ax.set_ylim([0, 1])\n",
        "            ax.grid(axis='y', alpha=0.3)\n",
        "            ax.set_xticklabels(models, rotation=15, ha='right')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\" Saved metric comparison to {save_path}\")\n",
        "        plt.close()\n",
        "\n",
        "    def plot_per_class_metrics(self, save_path='per_class_metrics.png'):\n",
        "        \"\"\"Plot precision, recall, F1 for each class across all models\"\"\"\n",
        "        emotion_classes = self.label_encoder.classes_\n",
        "        n_models = len(self.results)\n",
        "\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "        fig.suptitle('Per-Class Performance Metrics', fontsize=16, fontweight='bold')\n",
        "\n",
        "        metric_names = ['Precision', 'Recall', 'F1-Score']\n",
        "\n",
        "        for metric_idx, (ax, metric_name) in enumerate(zip(axes, metric_names)):\n",
        "            x = np.arange(len(emotion_classes))\n",
        "            width = 0.25\n",
        "\n",
        "            for model_idx, (model_name, result) in enumerate(self.results.items()):\n",
        "                y_true = self.trainer.y_test\n",
        "                y_pred = result['y_pred']\n",
        "\n",
        "                # Calculate per-class metrics\n",
        "                if metric_name == 'Precision':\n",
        "                    scores = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
        "                elif metric_name == 'Recall':\n",
        "                    scores = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
        "                else:  # F1-Score\n",
        "                    scores = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
        "\n",
        "                offset = width * (model_idx - 1)\n",
        "                ax.bar(x + offset, scores, width, label=model_name, alpha=0.8)\n",
        "\n",
        "            ax.set_xlabel('Emotion Class', fontsize=12, fontweight='bold')\n",
        "            ax.set_ylabel(metric_name, fontsize=12, fontweight='bold')\n",
        "            ax.set_title(f'{metric_name} by Emotion Class', fontsize=13, fontweight='bold')\n",
        "            ax.set_xticks(x)\n",
        "            ax.set_xticklabels(emotion_classes, rotation=45, ha='right')\n",
        "            ax.legend()\n",
        "            ax.grid(axis='y', alpha=0.3)\n",
        "            ax.set_ylim([0, 1])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"Saved per-class metrics to {save_path}\")\n",
        "        plt.close()\n",
        "\n",
        "    def plot_confusion_matrices(self, save_path='confusion_matrices.png'):\n",
        "        \"\"\"Plot confusion matrices for all models\"\"\"\n",
        "        n_models = len(self.results)\n",
        "        fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 5))\n",
        "\n",
        "        if n_models == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        emotion_classes = self.label_encoder.classes_\n",
        "\n",
        "        for ax, (model_name, result) in zip(axes, self.results.items()):\n",
        "            cm = confusion_matrix(self.trainer.y_test, result['y_pred'])\n",
        "\n",
        "            # Normalize confusion matrix\n",
        "            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "            sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
        "                       xticklabels=emotion_classes, yticklabels=emotion_classes,\n",
        "                       ax=ax, cbar_kws={'label': 'Proportion'})\n",
        "\n",
        "            ax.set_title(f'{model_name}\\nAccuracy: {result[\"accuracy\"]:.3f}',\n",
        "                        fontweight='bold', fontsize=12)\n",
        "            ax.set_xlabel('Predicted', fontweight='bold')\n",
        "            ax.set_ylabel('Actual', fontweight='bold')\n",
        "\n",
        "        plt.suptitle('Confusion Matrices (Normalized)', fontsize=16, fontweight='bold', y=1.02)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"Saved confusion matrices to {save_path}\")\n",
        "        plt.close()\n",
        "\n",
        "    def plot_classification_reports(self):\n",
        "        \"\"\"Print detailed classification reports\"\"\"\n",
        "\n",
        "        emotion_classes = self.label_encoder.classes_\n",
        "\n",
        "        for model_name, result in self.results.items():\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"{model_name}\")\n",
        "            print(f\"{'='*60}\")\n",
        "\n",
        "            y_true = self.trainer.y_test\n",
        "            y_pred = result['y_pred']\n",
        "\n",
        "            report = classification_report(\n",
        "                y_true, y_pred,\n",
        "                target_names=emotion_classes,\n",
        "                digits=3\n",
        "            )\n",
        "            print(report)\n",
        "\n",
        "    def plot_feature_importance(self, save_path='feature_importance.png', top_n=50):\n",
        "        \"\"\"Plot feature importance for tree-based models\"\"\"\n",
        "        tree_models = ['Random Forest', 'Gradient Boosting']\n",
        "        available_models = [m for m in tree_models if m in self.results]\n",
        "\n",
        "        fig, axes = plt.subplots(1, len(available_models), figsize=(10*len(available_models), 8))\n",
        "\n",
        "        if len(available_models) == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        for ax, model_name in zip(axes, available_models):\n",
        "            model = self.results[model_name]['model']\n",
        "            importances = model.feature_importances_\n",
        "\n",
        "            # Get top N features\n",
        "            indices = np.argsort(importances)[-top_n:]\n",
        "            feature_names = np.array(self.trainer.feature_names)[indices]\n",
        "\n",
        "            ax.barh(range(top_n), importances[indices], color='steelblue', alpha=0.8)\n",
        "            ax.set_yticks(range(top_n))\n",
        "            ax.set_yticklabels(feature_names, fontsize=9)\n",
        "            ax.set_xlabel('Feature Importance', fontweight='bold')\n",
        "            ax.set_title(f'{model_name}\\nTop {top_n} Features', fontweight='bold')\n",
        "            ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"\\n Saved feature importance to {save_path}\")\n",
        "        plt.close()\n",
        "\n",
        "    def generate_all_plots(self):\n",
        "        \"\"\"Generate all evaluation plots\"\"\"\n",
        "\n",
        "        self.plot_metric_comparison()\n",
        "        self.plot_per_class_metrics()\n",
        "        self.plot_confusion_matrices()\n",
        "        self.plot_feature_importance()\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution pipeline\"\"\"\n",
        "\n",
        "    # Configuration\n",
        "    features_csv = 'features_dataset.csv'\n",
        "    test_size = 0.2\n",
        "\n",
        "    # Load data\n",
        "    loader = EmotionDataLoader(features_csv)\n",
        "    loader.load_data().prepare_features()\n",
        "\n",
        "    # Train models\n",
        "    trainer = EmotionClassifierTrainer(\n",
        "        X=loader.X,\n",
        "        y=loader.y,\n",
        "        feature_names=loader.feature_names,\n",
        "        label_encoder=loader.label_encoder\n",
        "    )\n",
        "\n",
        "    trainer.split_data(test_size=test_size).train_all_models()\n",
        "\n",
        "    # Evaluate and visualize\n",
        "    evaluator = ModelEvaluator(trainer)\n",
        "    comparison_df = evaluator.print_comparison()\n",
        "    evaluator.plot_classification_reports()\n",
        "    evaluator.generate_all_plots()\n",
        "\n",
        "    return trainer, evaluator\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    trainer, evaluator = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6KnxWu1VISc",
        "outputId": "50873c0f-e573-4671-e5e9-a1a685cdc902"
      },
      "id": "Q6KnxWu1VISc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Loaded 1401 samples\n",
            "Total columns: 305\n",
            "\n",
            " Feature columns: 299\n",
            "\n",
            " Encoding emotion labels...\n",
            "\n",
            " Emotion classes:\n",
            "  0: Angry (200 samples)\n",
            "  1: Disgust (210 samples)\n",
            "  2: Fearful (216 samples)\n",
            "  3: Happy (216 samples)\n",
            "  4: Neutral (145 samples)\n",
            "  5: Sad (202 samples)\n",
            "  6: Surprise (212 samples)\n",
            "\n",
            " Training samples: 1120\n",
            " Test samples: 281\n",
            " Test size: 20%\n",
            "\n",
            "→ Training with 100 trees...\n",
            " Results of Random Forest Classifier\n",
            "  Accuracy: 0.5338\n",
            "\n",
            "→ Training with rbf kernel...\n",
            " Results of SVM Classifier\n",
            "  Accuracy: 0.5231\n",
            "\n",
            "→ Training with 100 estimators...\n",
            " Results of Gradient Boosting Classifier\n",
            "  Accuracy: 0.5196\n",
            "\n",
            "            Model  Accuracy  Precision   Recall  F1-Score\n",
            "    Random Forest  0.533808   0.534474 0.533808  0.532366\n",
            "              SVM  0.523132   0.539735 0.523132  0.525416\n",
            "Gradient Boosting  0.519573   0.527034 0.519573  0.520455\n",
            "\n",
            " Best Model: Random Forest (F1-Score: 0.5324)\n",
            "\n",
            "============================================================\n",
            "Random Forest\n",
            "============================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Angry      0.463     0.475     0.469        40\n",
            "     Disgust      0.444     0.476     0.460        42\n",
            "     Fearful      0.574     0.628     0.600        43\n",
            "       Happy      0.617     0.674     0.644        43\n",
            "     Neutral      0.690     0.690     0.690        29\n",
            "         Sad      0.625     0.488     0.548        41\n",
            "    Surprise      0.375     0.349     0.361        43\n",
            "\n",
            "    accuracy                          0.534       281\n",
            "   macro avg      0.541     0.540     0.539       281\n",
            "weighted avg      0.534     0.534     0.532       281\n",
            "\n",
            "\n",
            "============================================================\n",
            "SVM\n",
            "============================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Angry      0.368     0.350     0.359        40\n",
            "     Disgust      0.545     0.429     0.480        42\n",
            "     Fearful      0.667     0.605     0.634        43\n",
            "       Happy      0.571     0.744     0.646        43\n",
            "     Neutral      0.769     0.690     0.727        29\n",
            "         Sad      0.613     0.463     0.528        41\n",
            "    Surprise      0.310     0.419     0.356        43\n",
            "\n",
            "    accuracy                          0.523       281\n",
            "   macro avg      0.549     0.528     0.533       281\n",
            "weighted avg      0.540     0.523     0.525       281\n",
            "\n",
            "\n",
            "============================================================\n",
            "Gradient Boosting\n",
            "============================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Angry      0.359     0.350     0.354        40\n",
            "     Disgust      0.409     0.429     0.419        42\n",
            "     Fearful      0.692     0.628     0.659        43\n",
            "       Happy      0.593     0.744     0.660        43\n",
            "     Neutral      0.731     0.655     0.691        29\n",
            "         Sad      0.636     0.512     0.568        41\n",
            "    Surprise      0.326     0.349     0.337        43\n",
            "\n",
            "    accuracy                          0.520       281\n",
            "   macro avg      0.535     0.524     0.527       281\n",
            "weighted avg      0.527     0.520     0.520       281\n",
            "\n",
            " Saved metric comparison to metric_comparison.png\n",
            "Saved per-class metrics to per_class_metrics.png\n",
            "Saved confusion matrices to confusion_matrices.png\n",
            "\n",
            " Saved feature importance to feature_importance.png\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cv_proj",
      "language": "python",
      "name": "cv_proj"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}